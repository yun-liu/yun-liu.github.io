<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Yun Liu&#39;s home page">
    <link rel="shortcut icon" href="./images/logo-vision2.jpg">
    <link rel="stylesheet" href="./assets/jemdoc.css" type="text/css">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-88572407-1', 'auto');
      ga('send', 'pageview');
    </script>
    <meta name="google-site-verification" content="F0Q0t5oLq1pGwXGMf_38oA2MxW_zfiMRsQTYD4_GJoQ"/>
    <title>Yun Liu</title>
  </head>

  <body>
    <div id="layout-content" style="margin-top:1.5em">
      <table>
        <tbody>
          <tr>
            <td width="78%">
              <div id="toptitle">
                <h1>Yun Liu &nbsp;</h1>
              </div>
              <p>
                I am a professor at the <a href="https://cc.nankai.edu.cn/">College of Computer Science</a>, <a href="https://en.nankai.edu.cn/">Nankai University</a>.
                Previously, I served as a senior scientist at the <a href="https://www.a-star.edu.sg/i2r">Institute for Infocomm Research</a> (I2R), <a href="https://www.a-star.edu.sg/">A*STAR</a>.
                Prior to that, I was a postdoctoral researcher in the <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a> at <a href="https://ethz.ch/en.html">ETH Zurich</a>,
                working under the supervision of <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Prof. Luc Van Gool</a>.
                I obtained both my bachelor's and doctoral degrees from <a href="https://en.nankai.edu.cn/">Nankai University</a> in 2016 and 2020, respectively,
                under the guidance of <a href="https://mmcheng.net/cmm/">Prof. Ming-Ming Cheng</a>.
                My research interests focus on computer vision and deep learning.

                <br><br>
                <font color="red">实验室现招收2025年9月入学的博士生若干名，通过南开大学计算机学院的申请考核制或者本校校内转博，具体要求及申请流程详见学院官网上的
                <a href="https://cc.nankai.edu.cn/2024/1022/c13297a553598/page.htm">招生细则</a>。报名前请先发邮件与我联系。</font>

                <!-- <br><br>
                <font color="red">I am looking for motivated PhD students (co-supervised with <a href="https://personal.ntu.edu.sg/exdjiang/">Prof. Xudong Jiang</a> at NTU) and interns to work on image, video, and point cloud recognition problems.
                Scholarships are available [<a href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa">link</a>].
                Drop me an email if you are interested.</font> -->
              </p>
              <h3 style="padding-top:-0.32em"></h3>
              <object id="object" data="assets/envelope.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="mailto:liuyun@nankai.edu.cn">liuyun [AT] nankai.edu.cn</a>
              &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
              <object id="object" data="assets/scholar.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://scholar.google.com/citations?user=UB3doCoAAAAJ" target="_blank">Google Scholar</a>
              &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
              <object id="object" data="assets/github.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://github.com/yun-liu" target="_blank">GitHub</a>
            </td>
            <td>
              <img src="./images/love2.jpg" border="0" width="100%">
            </td>
          </tr>
        </tbody>
      </table>

      <!-- Selected Publications -->
      <h2>Selected Publications</h2>

      <ul>
        <div style="font-size: 0.8em">
          <strong>Notes:</strong> Joint first authors are indicated using # and corresponding authors are indicated using *.
        </div>

        <h3> 2025 </h3>

        <li>
          <p>
            <b>Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation</b><br>
            Zhaochong An, Guolei Sun*, <strong>Yun Liu*</strong>, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie<br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2025<br>
            [PDF]
            [Code]
            [Official Version]
          </p>
        </li>

        <li>
          <p>
            <b>Exploring Frequency-Inspired Optimization in Transformer for Efficient Single Image Super-Resolution</b><br>
            Ao Li, Le Zhang, <strong>Yun Liu</strong>, and Ce Zhu<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2025<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2025)Exploring%20Frequency-Inspired%20Optimization%20in%20Transformer%20for%20Efficient%20Single%20Image%20Super-Resolution.pdf">[PDF]</a>
            <a href="https://github.com/AVC2-UESTC/Frequency-Inspired-Optimization-for-EfficientSR.git">[Code]</a>
            [Official Version]
          </p>
        </li>

        <h3> 2024 </h3>

        <li>
          <p>
            <b>Learning Local and Global Temporal Contexts for Video Semantic Segmentation</b><br>
            Guolei Sun, <strong>Yun Liu*</strong>, Henghui Ding, Min Wu, and Luc Van Gool<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2024<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2024)Learning%20Local%20and%20Global%20Temporal%20Contexts%20for%20Video%20Semantic%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/GuoleiSun/VSS-CFFM">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/10496250">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>PSRR-MaxpoolNMS++: Fast Non-Maximum Suppression with Discretization and Pooling</b><br>
            Tianyi Zhang, Chunyun Chen, <strong>Yun Liu*</strong>, Xue Geng, Mohamed M. Sabry Aly, and Jie Lin<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2024<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2024)PSRR-MaxpoolNMS%2B%2B%20-%20Fast%20Non-Maximum%20Suppression%20with%20Discretization%20and%20Pooling.pdf">[PDF]</a>
            <a href="https://ieeexplore.ieee.org/document/10736991">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Rethinking Few-shot 3D Point Cloud Semantic Segmentation</b><br>
            Zhaochong An, Guolei Sun*, <strong>Yun Liu*</strong>, Fayao Liu, Zongwei Wu, Dan Wang, Luc Van Gool, and Serge Belongie<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2024<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2024)Rethinking%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/ZhaochongAn/COSeg">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2024/html/An_Rethinking_Few-shot_3D_Point_Cloud_Semantic_Segmentation_CVPR_2024_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Vision Transformers with Hierarchical Attention</b><br>
            <i>(First titled <a href="https://arxiv.org/abs/2106.03180v2">"Transformer in Convolutional Neural Networks"</a>)</i><br>
            <strong>Yun Liu</strong>, Yu-Huan Wu, Guolei Sun, Le Zhang, Ajad Chhatkuli, and Luc Van Gool<br>
            <i>Machine Intelligence Research (MIR)</i>, 2024<br>
            <a href="https://yun-liu.github.io/papers/(MIR'2024)Vision%20Transformers%20with%20Hierarchical%20Attention.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/HAT-Net">[Code]</a>
            <a href="https://link.springer.com/article/10.1007/s11633-024-1393-8">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Rethinking Global Context in Crowd Counting</b><br>
            <i>(First titled <a href="https://arxiv.org/abs/2105.10926v1">"Boosting Crowd Counting with Transformers"</a>)</i><br>
            Guolei Sun, <strong>Yun Liu*</strong>, Thomas Probst, Danda Pani Paudel, Nikola Popovic, and Luc Van Gool<br>
            <i>Machine Intelligence Research (MIR)</i>, 2024<br>
            <a href="https://yun-liu.github.io/papers/(MIR'2023)Rethinking%20Global%20Context%20in%20Crowd%20Counting.pdf">[PDF]</a>
            <a href="https://link.springer.com/article/10.1007/s11633-023-1475-z">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Towards Open-Vocabulary Video Semantic Segmentation</b><br>
            Xinhao Li, <strong>Yun Liu</strong>, Guolei Sun, Min Wu, Le Zhang, and Ce Zhu<br>
            <i>IEEE Transactions on Multimedia (TMM)</i>, 2024<br>
            <a href="https://yun-liu.github.io/papers/(TMM'2024)Towards%20Open-Vocabulary%20Video%20Semantic%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/AVC2-UESTC/OV2VSS">[Code]</a>
            [Official Version]
          </p>
        </li>

        <h3> 2023 </h3>

        <li>
          <p>
            <b>Revisiting Computer-Aided Tuberculosis Diagnosis</b><br>
            <strong>Yun Liu</strong>, Yu-Huan Wu, Shi-Chen Zhang, Li Liu, Min Wu, and Ming-Ming Cheng<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2023)Revisiting%20Computer-Aided%20Tuberculosis%20Diagnosis.pdf">[PDF]</a>
            <a href="https://mmcheng.net/tb/">[Project Page]</a>
            <a href="https://github.com/yun-liu/Tuberculosis">[Code]</a>
            <a href="https://drive.google.com/file/d/1r-oNYTPiPCOUzSjChjCIYTdkjBTugqxR/view?usp=sharing">[Dataset on Google Drive]</a>
            <a href="https://pan.baidu.com/s/1INhqaZyPFKWPFXgynerXew">[Dataset on Baidu Yunpan]</a>
            <a href="https://yun-liu.github.io/materials/TPAMI2023_Tuberculosis_CN.pdf">[中译版]</a>
            <a href="https://codalab.lisn.upsaclay.fr/competitions/7916">[<strong>Online Challenge</strong>]</a>
            <a href="https://ieeexplore.ieee.org/document/10310292">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution</b><br>
            Ao Li, Le Zhang, <strong>Yun Liu</strong>, and Ce Zhu<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2023<br>
            <a href="https://yun-liu.github.io/papers/(ICCV'2023)Feature%20Modulation%20Transformer%20-%20Cross-Refinement%20of%20Global%20Representation%20via%20High-Frequency%20Prior%20for%20Image%20Super-Resolution.pdf">[PDF]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2023_CRAFT_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://github.com/AVC2-UESTC/CRAFT-SR">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Feature_Modulation_Transformer_Cross-Refinement_of_Global_Representation_via_High-Frequency_Prior_ICCV_2023_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Indiscernible Object Counting in Underwater Scenes</b><br>
            Guolei Sun, Zhaochong An, <strong>Yun Liu</strong>, Ce Liu, Christos Sakaridis, Deng-Ping Fan, and Luc Van Gool<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2023<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2023)Indiscernible%20Object%20Counting%20in%20Underwater%20Scenes.pdf">[PDF]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2023_Counting_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://github.com/GuoleiSun/Indiscernible-Object-Counting">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Indiscernible_Object_Counting_in_Underwater_Scenes_CVPR_2023_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net</b><br>
            Yu Qiu, <strong>Yun Liu*</strong>, Le Zhang, and Jing Xu*<br>
            <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</i>, 2023<br>
            <a href="https://yun-liu.github.io/papers/(TCSVT'2023)Boosting%20Salient%20Object%20Detection%20with%20Transformer-based%20Asymmetric%20Bilateral%20U-Net.pdf">[PDF]</a>
            <a href="https://github.com/yuqiuyuqiu/abiu-net">[Code]</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/10227346">[Official Version]</a>
          </p>
        </li>

        <h3> 2022 </h3>

        <li>
          <p>
            <b>Mining Relations among Cross-Frame Affinities for Video Semantic Segmentation</b><br>
            Guolei Sun, <strong>Yun Liu*</strong>, Hao Tang, Ajad Chhatkuli, Le Zhang, and Luc Van Gool<br>
            <i>European Conference on Computer Vision (ECCV)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(ECCV'2022)Mining%20Relations%20among%20Cross-Frame%20Affinities%20for%20Video%20Semantic%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/guoleisun/VSS-MRCFA">[Code]</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-19830-4_30">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Coarse-to-Fine Feature Mining for Video Semantic Segmentation</b><br>
            Guolei Sun, <strong>Yun Liu*</strong>, Henghui Ding, Thomas Probst, and Luc Van Gool<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2022)Coarse-to-Fine%20Feature%20Mining%20for%20Video%20Semantic%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/GuoleiSun/VSS-CFFM">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_Coarse-To-Fine_Feature_Mining_for_Video_Semantic_Segmentation_CVPR_2022_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>P2T: Pyramid Pooling Transformer for Scene Understanding</b><br>
            Yu-Huan Wu<sup>#</sup>, <strong>Yun Liu<sup>#</sup></strong>, Xin Zhan, and Ming-Ming Cheng<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2022)P2T%20-%20Pyramid%20Pooling%20Transformer%20for%20Scene%20Understanding.pdf">[PDF]</a>
            <a href="https://github.com/yuhuan-wu/P2T">[Code]</a>
            <a href="https://yun-liu.github.io/materials/TPAMI2022_P2T_CN.pdf">[中译版]</a>
            <a href="https://ieeexplore.ieee.org/document/9870559">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>EDN: Salient Object Detection via Extremely-Downsampled Network</b><br>
            Yu-Huan Wu<sup>#</sup>, <strong>Yun Liu<sup>#</sup></strong>, Le Zhang, Ming-Ming Cheng, and Bo Ren<br>
            <i>IEEE Transactions on Image Processing (TIP)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(TIP'2022)EDN%20-%20Salient%20Object%20Detection%20via%20Extremely-Downsampled%20Network.pdf">[PDF]</a>
            <a href="https://github.com/yuhuan-wu/EDN">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9756227">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>MiniSeg: An Extremely Minimum Network Based on Lightweight Multiscale Learning for Efficient COVID-19 Segmentation</b><br>
            Yu Qiu, <strong>Yun Liu*</strong>, Shijie Li, and Jing Xu*<br>
            <i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(TNNLS'2022)MiniSeg%20-%20An%20Extremely%20Minimum%20Network%20Based%20on%20Lightweight%20Multiscale%20Learning%20for%20Efficient%20COVID-19%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/MiniSeg">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/10003166">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>A2SPPNet: Attentive Atrous Spatial Pyramid Pooling Network for Salient Object Detection</b><br>
            Yu Qiu, <strong>Yun Liu*</strong>, Yanan Chen, Jianwen Zhang, Jinchao Zhu, and Jing Xu*<br>
            <i>IEEE Transactions on Multimedia (TMM)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(TMM'2022)A2SPPNet%20-%20Attentive%20Atrous%20Spatial%20Pyramid%20Pooling%20Network%20for%20Salient%20Object%20Detection.pdf">[PDF]</a>
            <a href="https://ieeexplore.ieee.org/document/9678052">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Zero Pixel Directional Boundary by Vector Transform</b><br>
            Edoardo Mello Rella, Ajad Chhatkuli, <strong>Yun Liu</strong>, Ender Konukoglu, and Luc Van Gool<br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2022<br>
            <a href="https://yun-liu.github.io/papers/(ICLR'2022)Zero%20Pixel%20Directional%20Boundary%20by%20Vector%20Transform.pdf">[PDF]</a>
            <a href="https://openreview.net/pdf?id=nxcABL7jbQh">[Official Version]</a>
          </p>
        </li>

        <h3> 2021 </h3>

        <li>
          <p>
            <b>Semantic Edge Detection with Diverse Deep Supervision</b><br>
            <strong>Yun Liu</strong>, Ming-Ming Cheng, Deng-Ping Fan, Le Zhang, Jia-Wang Bian, and Dacheng Tao<br>
            <i>International Journal of Computer Vision (IJCV)</i>, 2021<br>
            <a href="https://yun-liu.github.io/papers/(IJCV'2021)Semantic%20Edge%20Detection%20with%20Diverse%20Deep%20Supervision.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/DDS">[Code]</a>
            <a href="https://yun-liu.github.io/materials/IJCV2021_DDS_CN.pdf">[中译版]</a>
            <a href="https://link.springer.com/article/10.1007/s11263-021-01539-8">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>SAMNet: Stereoscopically Attentive Multi-scale Network for Lightweight Salient Object Detection</b><br>
            <strong>Yun Liu<sup>#</sup></strong>, Xin-Yu Zhang<sup>#</sup>, Jia-Wang Bian, Le Zhang, and Ming-Ming Cheng<br>
            <i>IEEE Transactions on Image Processing (TIP)</i>, 2021<br>
            <a href="https://yun-liu.github.io/papers/(TIP'2021)SAMNet%20-%20Stereoscopically%20Attentive%20Multi-scale%20Network%20for%20Lightweight%20Salient%20Object%20Detection.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/FastSaliency">[Code]</a>
            <a href="https://yun-liu.github.io/materials/TIP2021_SAMNet_CN.pdf">[中译版]</a>
            <a href="https://ieeexplore.ieee.org/document/9381668">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>DNA: Deeply-supervised Nonlinear Aggregation for Salient Object Detection</b><br>
            <strong>Yun Liu</strong>, Ming-Ming Cheng, Xin-Yu Zhang, Guang-Yu Nie, and Meng Wang<br>
            <i>IEEE Transactions on Cybernetics (TCYB)</i>, 2021<br>
            <a href="https://yun-liu.github.io/papers/(TCYB'2021)DNA%20-%20Deeply-supervised%20Nonlinear%20Aggregation%20for%20Salient%20Object%20Detection.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/DNA">[Saliency Maps]</a>
            <a href="https://yun-liu.github.io/materials/TCYB2021_DNA_CN.pdf">[中译版]</a>
            <a href="https://ieeexplore.ieee.org/document/9345433">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>MiniSeg: An Extremely Minimum Network for Efficient COVID-19 Segmentation</b><br>
            Yu Qiu, <strong>Yun Liu*</strong>, Shijie Li, and Jing Xu*<br>
            <i>AAAI Conference on Artificial Intelligence (AAAI)</i>, 2021<br>
            <a href="https://yun-liu.github.io/papers/(AAAI'2021)MiniSeg%20-%20An%20Extremely%20Minimum%20Network%20for%20Efficient%20COVID-19%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/MiniSeg">[Code]</a>
            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16617">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>MobileSal: Extremely Efficient RGB-D Salient Object Detection</b><br>
            Yu-Huan Wu, <strong>Yun Liu</strong>, Jun Xu, Jia-Wang Bian, Yuchao Gu, and Ming-Ming Cheng<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2021<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2021)MobileSal%20-%20Extremely%20Efficient%20RGB-D%20Salient%20Object%20Detection.pdf">[PDF]</a>
            <a href="https://mmcheng.net/mobilesal/">[Project Page]</a>
            <a href="https://github.com/yuhuan-wu/MobileSal">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9647954">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>DOTS: Decoupling Operation and Topology in Differentiable Architecture Search</b><br>
            Yuchao Gu, Lijuan Wang, <strong>Yun Liu</strong>, Yi Yang, Yu-Huan Wu, Shao-Ping Lu, and Ming-Ming Cheng<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2021<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2021)DOTS%20-%20Decoupling%20Operation%20and%20Topology%20in%20Differentiable%20Architecture%20Search.pdf">[PDF]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2021_DOTS_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://github.com/guyuchao/DOTS">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Gu_DOTS_Decoupling_Operation_and_Topology_in_Differentiable_Architecture_Search_CVPR_2021_paper.html">[Official Version]</a>
          </p>
        </li>

        <h3> 2020 </h3>

        <li>
          <p>
            <b>Leveraging Instance-, Image- and Dataset-Level Information for Weakly Supervised Instance Segmentation</b><br>
            <strong>Yun Liu<sup>#</sup></strong>, Yu-Huan Wu<sup>#</sup>, Peisong Wen, Yujun Shi, Yu Qiu, and Ming-Ming Cheng<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2020)Leveraging%20Instance-%2C%20Image-%20and%20Dataset-Level%20Information%20for%20Weakly%20Supervised%20Instance%20Segmentation.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/LIID">[Code]</a>
            <a href="https://yun-liu.github.io/materials/TPAMI2020_LIID_CN.pdf">[中译版]</a>
            <a href="https://ieeexplore.ieee.org/document/9193980">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Rethinking Computer-aided Tuberculosis Diagnosis</b><br>
            <strong>Yun Liu<sup>#</sup></strong>, Yu-Huan Wu<sup>#</sup>, Yunfeng Ban, Huifang Wang, and Ming-Ming Cheng<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR, <strong>Oral</strong>)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2020)Rethinking%20Computer-aided%20Tuberculosis%20Diagnosis.pdf">[PDF]</a>
            <a href="https://mmcheng.net/tb/">[Project Page]</a>
            <a href="https://drive.google.com/file/d/1r-oNYTPiPCOUzSjChjCIYTdkjBTugqxR/view?usp=sharing">[Dataset on Google Drive]</a>
            <a href="https://pan.baidu.com/s/1INhqaZyPFKWPFXgynerXew">[Dataset on Baidu Yunpan]</a>
            <a href="https://codalab.lisn.upsaclay.fr/competitions/7916">[<strong>Online Challenge</strong>]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2020_Tuberculosis_CN.pdf">[中译版]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2020_Tuberculosis_Oral_Video.mp4">[Video]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2020_Tuberculosis_PPT.pdf">[PPT]</a>
            <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Rethinking_Computer-Aided_Tuberculosis_Diagnosis_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Lightweight Salient Object Detection via Hierarchical Visual Perception Learning</b><br>
            <strong>Yun Liu<sup>#</sup></strong>, Yu-Chao Gu<sup>#</sup>, Xin-Yu Zhang<sup>#</sup>, Weiwei Wang, and Ming-Ming Cheng<br>
            <i>IEEE Transactions on Cybernetics (TCYB)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(TCYB'2020)Lightweight%20Salient%20Object%20Detection%20via%20Hierarchical%20Visual%20Perception%20Learning.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/FastSaliency">[Code]</a>
            <a href="https://yun-liu.github.io/materials/TCYB2020_HVPNet_CN.pdf">[中译版]</a>
            <a href="https://ieeexplore.ieee.org/document/9285193">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation</b><br>
            Shijie Li, Yazan Abu Farha, <strong>Yun Liu</strong>, Ming-Ming Cheng, and Juergen Gall<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2020)MS-TCN%2B%2B%20-%20Multi-Stage%20Temporal%20Convolutional%20Network%20for%20Action%20Segmentation.pdf">[PDF]</a>
            <a href="https://mmcheng.net/ms-tcn/">[Project Page]</a>
            <a href="https://github.com/sj-li/MS-TCN2">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9186840">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Ordered or Orderless: A Revisit for Video based Person Re-Identification</b><br>
            Le Zhang, Zenglin Shi, Joey Tianyi Zhou, Ming-Ming Cheng, <strong>Yun Liu</strong>, Jia-Wang Bian, Zeng Zeng, and Chunhua Shen<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2020)Ordered%20or%20Orderless%20-%20A%20Revisit%20for%20Video%20based%20Person%20Re-Identification.pdf">[PDF]</a>
            <a href="https://github.com/ZhangLeUestc/VideoReid-TPAMI2020">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9018082">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence</b><br>
            Jia-Wang Bian, Wen-Yan Lin, <strong>Yun Liu</strong>, Le Zhang, Sai-Kit Yeung, Ming-Ming Cheng, and Ian Reid<br>
            <i>International Journal of Computer Vision (IJCV)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(IJCV'2020)GMS%20-%20Grid-based%20Motion%20Statistics%20for%20Fast%2C%20Ultra-robust%20Feature%20Correspondence.pdf">[PDF]</a>
            <a href="http://jwbian.net/gms">[Project Page]</a>
            <a href="https://github.com/JiawangBian/GMS-Feature-Matcher">[Code]</a>
            <a href="https://yun-liu.github.io/materials/IJCV2020_GMS_Video.mp4">[Video]</a>
            <a href="https://link.springer.com/article/10.1007%2Fs11263-019-01280-3">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection</b><br>
            Yuchao Gu, Lijuan Wang, Ziqin Wang, <strong>Yun Liu</strong>, Ming-Ming Cheng, and Shao-Ping Lu<br>
            <i>AAAI Conference on Artificial Intelligence (AAAI)</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(AAAI'2020)Pyramid%20Constrained%20Self-Attention%20Network%20for%20Fast%20Video%20Salient%20Object%20Detection.pdf">[PDF]</a>
            <a href="http://mmcheng.net/pcsa/">[Project Page]</a>
            <a href="https://github.com/guyuchao/PyramidCSA">[Code]</a>
            <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6718">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>RefinedBox: Refining for Fewer and High-quality Object Proposals</b><br>
            <strong>Yun Liu</strong>, Shi-Jie Li, and Ming-Ming Cheng<br>
            <i>Neurocomputing</i>, 2020<br>
            <a href="https://yun-liu.github.io/papers/(Neurocomputing'2020)RefinedBox%20-%20Refining%20for%20Fewer%20and%20High-quality%20Object%20Proposals.pdf">[PDF]</a>
            <a href="https://github.com/yun-liu/RefinedBox">[Code]</a>
            <a href="https://yun-liu.github.io/materials/Neurocomputing2020_RefinedBox_CN.pdf">[中译版]</a>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220305816">[Official Version]</a>
          </p>
        </li>

        <h3> 2019 </h3>

        <li>
          <p>
            <b>Richer Convolutional Features for Edge Detection</b><br>
            <strong>Yun Liu</strong>, Ming-Ming Cheng, Xiaowei Hu, Jia-Wang Bian, Le Zhang, Xiang Bai, and Jinhui Tang<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2019<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2019)Richer%20Convolutional%20Features%20for%20Edge%20Detection.pdf">[PDF]</a>
            <a href="https://mmcheng.net/rcfedge/">[Project Page]</a>
            <a href="https://github.com/yun-liu/rcf">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/8516362">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Nonlinear Regression via Deep Negative Correlation Learning</b><br>
            Le Zhang, Zenglin Shi, Ming-Ming Cheng, <strong>Yun Liu</strong>, Jia-Wang Bian, Joey Tianyi Zhou, Guoyan Zheng, and Zeng Zeng<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2019<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2019)Nonlinear%20Regression%20via%20Deep%20Negative%20Correlation%20Learning.pdf">[PDF]</a>
            <a href="https://mmcheng.net/dncl/">[Project Page]</a>
            <a href="https://github.com/shizenglin/Deep-NCL">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/8850209">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Scoot: A Perceptual Metric for Facial Sketches</b><br>
            Deng-Ping Fan, ShengChuan Zhang, Yu-Huan Wu, <strong>Yun Liu</strong>, Ming-Ming Cheng, Bo Ren, Paul Rosin, and Rongrong Ji<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2019<br>
            <a href="https://yun-liu.github.io/papers/(ICCV'2019)Scoot%20-%20A%20Perceptual%20Metric%20for%20Facial%20Sketches.pdf">[PDF]</a>
            <a href="http://dpfan.net/Scoot/">[Project Page]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2019_Scoot_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://github.com/DengPingFan/Scoot">[Code]</a>
            <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Multi-Level Context Ultra-Aggregation for Stereo Matching</b><br>
            Guang-Yu Nie, Ming-Ming Cheng, <strong>Yun Liu</strong>, Zhengfa Liang, Deng-Ping Fan, Yue Liu, and Yongtian Wang<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2019<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2019)Multi-Level%20Context%20Ultra-Aggregation%20for%20Stereo%20Matching.pdf">[PDF]</a>
            <a href="https://mmcheng.net/mcua/">[Project Page]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2019_MCUA_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2019_MCUA_PPT.pdf">[PPT]</a>
            <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Nie_Multi-Level_Context_Ultra-Aggregation_for_Stereo_Matching_CVPR_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>BING: Binarized Normed Gradients for Objectness Estimation at 300fps</b><br>
            Ming-Ming Cheng<sup>#</sup>, <strong>Yun Liu<sup>#</sup></strong>, Wen-Yan Lin, Ziming Zhang,  Paul L. Rosin, and Philip Torr<br>
            <i>Computational Visual Media (CVMJ)</i>, 2019<br>
            <a href="https://yun-liu.github.io/papers/(CVMJ'2019)BING%20-%20Binarized%20Normed%20Gradients%20for%20Objectness%20Estimation%20at%20300fps.pdf">[PDF]</a>
            <a href="https://mmcheng.net/zh/bing/">[Project Page]</a>
            <a href="https://github.com/yun-liu/bing">[Code]</a>
            <a href="https://link.springer.com/article/10.1007/s41095-018-0120-1">[Official Version]</a>
          </p>
        </li>

        <h3> 2016 - 2018 </h3>
        <li>
          <p>
            <b>DEL: Deep Embedding Learning for Efficient Image Segmentation</b><br>
            <strong>Yun Liu</strong>, Peng-Tao Jiang, Vahan Petrosyan, Shi-Jie Li, Jiawang Bian, Le Zhang, and Ming-Ming Cheng<br>
            <i>International Joint Conference on Artificial Intelligence (IJCAI)</i>, 2018<br>
            <a href="https://yun-liu.github.io/papers/(IJCAI'2018)DEL%20-%20Deep%20Embedding%20Learning%20for%20Efficient%20Image%20Segmentation.pdf">[PDF]</a>
            <a href="https://mmcheng.net/del/">[Project Page]</a>
            <a href="https://github.com/yun-liu/del">[Code]</a>
            <a href="https://yun-liu.github.io/materials/IJCAI2018_DEL_CN.pdf">[中译版]</a>
            <a href="https://www.ijcai.org/proceedings/2018/120">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Crowd Counting with Deep Negative Correlation Learning</b><br>
            Zenglin Shi, Le Zhang, <strong>Yun Liu</strong>, XiaoFeng Cao, Yangdong Ye, Ming-Ming Cheng, and Guoyan Zheng<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2018<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2018)Crowd%20Counting%20with%20Deep%20Negative%20Correlation%20Learning.pdf">[PDF]</a>
            <a href="https://sites.google.com/site/zhangleuestc/crowd-counting-with-deep-negative-learning/crowd-counting-with-deep-negative-learning">[Project Page]</a>
            <a href="https://github.com/shizenglin/Deep-NCL">[Code]</a>
            <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Crowd_Counting_via_CVPR_2018_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Sequential Optimization for Efficient High-Quality Object Proposal Generation</b><br>
            Ziming Zhang, <strong>Yun Liu</strong>, Xi Chen, Yanjun Zhu, Ming-Ming Cheng, Venkatesh Saligrama, and Philip H.S. Torr<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2017<br>
            <a href="https://yun-liu.github.io/papers/(TPAMI'2017)Sequential%20Optimization%20for%20Efficient%20High-Quality%20Object%20Proposal%20Generation.pdf">[PDF]</a>
            <!-- <a href="https://pan.baidu.com/s/1slEK7Q9?errno=0&errmsg=Auth%20Login%20Sucess&&bduss=&ssnerror=0">[Code]</a> -->
            <a href="https://github.com/Zhang-VISLab/Sequential-Optimization-for-Efficient-High-Quality-Object-Proposal-Generation">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/7932893">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Structure-measure: A New Way to Evaluate Foreground Maps</b><br>
            DengPing Fan, Ming-Ming Cheng, <strong>Yun Liu</strong>, Tao Li, and Ali Borji<br>
            <i>International Conference on Computer Vision (ICCV, <strong>Spotlight</strong>)</i>, 2017<br>
            <a href="https://yun-liu.github.io/papers/(ICCV'2017)Structure-measure%20-%20A%20New%20Way%20to%20Evaluate%20Foreground%20Maps.pdf">[PDF]</a>
            <a href="http://dpfan.net/smeasure/">[Project Page]</a>
            <a href="https://github.com/DengPingFan/S-measure">[Code]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2017_S-measure_PPT.pdf">[PPT]</a>
            <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Fan_Structure-Measure_A_New_ICCV_2017_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Richer Convolutional Features for Edge Detection</b><br>
            <strong>Yun Liu</strong>, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, and Xiang Bai<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2017<br>
            <a href="https://yun-liu.github.io/papers/(CVPR'2017)Richer%20Convolutional%20Features%20for%20Edge%20Detection.pdf">[PDF]</a>
            <a href="https://mmcheng.net/rcfedge/">[Project Page]</a>
            <a href="https://github.com/yun-liu/rcf">[Code]</a>
            <a href="https://yun-liu.github.io/materials/CVPR2017_RCF_CN.pdf">[中译版]</a>
            <a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Richer_Convolutional_Features_CVPR_2017_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>HFS: Hierarchical Feature Selection for Efficient Image Segmentation</b><br>
            Ming-Ming Cheng<sup>#</sup>, <strong>Yun Liu<sup>#</sup></strong>, Qibin Hou, Jiawang Bian, Philip Torr, Shi-Min Hu, and Zhuowen Tu<br>
            <i>European Conference on Computer Vision (ECCV)</i>, 2016<br>
            <a href="https://yun-liu.github.io/papers/(ECCV'2016)HFS%20-%20Hierarchical%20Feature%20Selection%20for%20Efficient%20Image%20Segmentation.pdf">[PDF]</a>
            <a href="https://mmcheng.net/hfs/">[Project Page]</a>
            <a href="https://github.com/yun-liu/hfs">[Code]</a>
            <a href="https://yun-liu.github.io/materials/ECCV2016_HFS_CN.pdf">[中译版]</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_53">[Official Version]</a>
          </p>
        </li>
      </ul>

      <div id="footer">
        <div id="footer-text">© Yun Liu</div>
      </div>

    </div>
  </body>
</html>
